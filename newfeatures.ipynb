{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor, GradientBoostingClassifier\n",
    "from sklearn.model_selection import KFold, cross_val_score, StratifiedKFold\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.feature_selection import mutual_info_regression\n",
    "from xgboost import XGBRegressor, XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((30471, 292), (7662, 291))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_raw = pd.read_csv('/Volumes/KM/Archive/KaggleStuff/2017SBER/train.csv')\n",
    "df_test_raw = pd.read_csv('/Volumes/KM/Archive/KaggleStuff/2017SBER/test.csv')\n",
    "df_macro_data = pd.read_csv('/Volumes/KM/Archive/KaggleStuff/2017SBER/macro.csv')\n",
    "df_macro_euusd = df_macro_data[['timestamp', 'usdrub', 'eurrub']]\n",
    "\n",
    "df_train_raw.shape, df_test_raw.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def time_year(x):\n",
    "    year = x.split('-')\n",
    "    return int(year[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def time_month(x):\n",
    "    month = x.split('-')\n",
    "    return int(month[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conv_to_bool(x):\n",
    "    if x == 'yes':\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_train_raw = pd.merge(df_train_raw, df_macro_data, on='timestamp')\n",
    "df_test_raw = pd.merge(df_test_raw, df_macro_data, on='timestamp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train:  1\n",
      "test:  0\n"
     ]
    }
   ],
   "source": [
    "df_train = pd.DataFrame()\n",
    "df_test = pd.DataFrame()\n",
    "\n",
    "feature_cols = ['id', 'full_sq', 'sport_count_5000', 'sport_count_3000', 'trc_count_5000', 'zd_vokzaly_avto_km',\n",
    "                'sadovoe_km', 'sport_count_2000', 'bulvar_ring_km', 'kremlin_km', 'ttk_km', 'trc_sqm_5000',\n",
    "                'nuclear_reactor_km', 'sport_count_1500', 'office_sqm_5000', 'sport_objects_raion', 'trc_count_3000',\n",
    "                'stadium_km', 'cafe_count_5000_price_1000', 'detention_facility_km', 'basketball_km',\n",
    "                'cafe_count_5000_price_1500', 'office_km', 'cafe_count_5000', 'cafe_count_5000_na_price',\n",
    "                'university_km', 'trc_sqm_3000', 'cafe_count_5000_price_500', 'workplaces_km',\n",
    "                'cafe_count_5000_price_2500', 'office_sqm_3000', 'swim_pool_km', 'thermal_power_plant_km',\n",
    "                'office_count_5000', 'catering_km', 'exhibition_km', 'church_count_5000', 'office_sqm_2000',\n",
    "                'cafe_count_5000_price_high', 'cafe_count_5000_price_4000', 'big_church_km',\n",
    "                'school_education_centers_raion', 'sport_count_1000', 'fitness_km', 'metro_min_avto',\n",
    "                'market_count_5000', 'park_km', 'big_church_count_5000', 'leisure_count_5000',\n",
    "                'office_sqm_1500', 'ekder_male', 'metro_km_avto', 'trc_count_2000', 'shopping_centers_km',\n",
    "                'public_healthcare_km', 'ekder_all', 'ekder_female', 'cafe_count_3000_price_1000',\n",
    "                'office_count_1500', 'raion_popul', 'usdrub', 'eurrub', 'cafe_count_2000', 'theater_km',\n",
    "                'office_raion', 'indust_part']\n",
    "\n",
    "\n",
    "for col in feature_cols:\n",
    "    df_train[col] = df_train_raw[col]\n",
    "    df_test[col] = df_test_raw[col]\n",
    "\n",
    "\n",
    "df_train['sale_year'] = df_train_raw['timestamp'].apply(time_year)\n",
    "df_train['sale_month'] = df_train_raw['timestamp'].apply(time_month)\n",
    "\n",
    "df_test['sale_year'] = df_test_raw['timestamp'].apply(time_year)\n",
    "df_test['sale_month'] = df_test_raw['timestamp'].apply(time_month)\n",
    "\n",
    "df_train['price'] = df_train_raw['price_doc']\n",
    "df_train['price_eur'] = df_train_raw['price_doc'] / df_train_raw['eurrub']\n",
    "df_train['price_usd'] = df_train_raw['price_doc'] / df_train_raw['usdrub']\n",
    "\n",
    "\n",
    "nan_data = df_train['full_sq'] > 2000\n",
    "print('train: ', nan_data.sum())\n",
    "df_train = df_train[~nan_data]\n",
    "\n",
    "nan_data = df_test['full_sq'] > 2000\n",
    "print('test: ', nan_data.sum())\n",
    "df_test = df_test[~nan_data]\n",
    "\n",
    "df_train.to_csv('/Volumes/KM/Archive/KaggleStuff/2017SBER/transformed_data/new/train_v0.csv', index=False)\n",
    "df_test.to_csv('/Volumes/KM/Archive/KaggleStuff/2017SBER/transformed_data/new/test_v0.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean(dft, name='life_sq_lg1'):\n",
    "    dft.loc[dft['full_sq'] < dft[name],name] = dft['full_sq'][dft['full_sq'] < dft[name]]\n",
    "    return dft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('/Volumes/KM/Archive/KaggleStuff/2017SBER/transformed_data/new/train_v0.csv')\n",
    "df_test = pd.read_csv('/Volumes/KM/Archive/KaggleStuff/2017SBER/transformed_data/new/test_v0.csv')\n",
    "\n",
    "df_train = pd.merge(df_train, df_train_raw[['id', 'life_sq']], on='id')\n",
    "df_test = pd.merge(df_test, df_test_raw[['id', 'life_sq']], on='id')   \n",
    "\n",
    "df_train['life_sq_lg1'] = df_train['life_sq']\n",
    "df_test['life_sq_lg1'] = df_test['life_sq']\n",
    "\n",
    "df_train = clean(df_train)\n",
    "df_test = clean(df_test)\n",
    "\n",
    "\n",
    "nan_ind = ~pd.isnull(df_train['life_sq_lg1'])\n",
    "\n",
    "lgreg_life_sq = LinearRegression(fit_intercept=True)\n",
    "\n",
    "tmp_feat_arr = np.zeros([df_train['full_sq'][nan_ind].values.shape[0], 1])\n",
    "tmp_feat_arr[:, 0] = df_train['full_sq'][nan_ind].values[:]\n",
    "tmp_targ_arr = np.reshape(df_train['life_sq_lg1'][nan_ind].values,\n",
    "                          (df_train['life_sq_lg1'][nan_ind].values.shape[0], 1))\n",
    "lgreg_life_sq.fit(tmp_feat_arr, np.log(tmp_targ_arr + 1))\n",
    "tmp_feat_arr = np.zeros([df_train['full_sq'][~nan_ind].values.shape[0], 1])\n",
    "tmp_feat_arr[:, 0] = df_train['full_sq'][~nan_ind].values[:]\n",
    "\n",
    "pred_lsq = lgreg_life_sq.predict(tmp_feat_arr)\n",
    "df_train.loc[~nan_ind, 'life_sq_lg1'] = np.exp(pred_lsq[:, 0]) - 1\n",
    "df_train.loc[df_train['life_sq_lg1'] < 0, 'life_sq_lg1'] = 0\n",
    "\n",
    "nan_ind = ~pd.isnull(df_test['life_sq'])\n",
    "tmp_feat_arr = np.zeros([df_test['full_sq'][~nan_ind].values.shape[0], 1])\n",
    "tmp_feat_arr[:, 0] = df_test['full_sq'][~nan_ind].values[:]\n",
    "pred_lsq = lgreg_life_sq.predict(tmp_feat_arr)\n",
    "df_test.loc[~nan_ind, 'life_sq_lg1'] = np.exp(pred_lsq[:, 0]) - 1\n",
    "df_test.loc[df_train['life_sq_lg1'] < 0, 'life_sq_lg1'] = 0\n",
    "\n",
    "df_train.to_csv('/Volumes/KM/Archive/KaggleStuff/2017SBER/transformed_data/new/train_v1.csv', index=False)\n",
    "df_test.to_csv('/Volumes/KM/Archive/KaggleStuff/2017SBER/transformed_data/new/test_v1.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('/Volumes/KM/Archive/KaggleStuff/2017SBER/transformed_data/new/train_v1.csv')\n",
    "df_test = pd.read_csv('/Volumes/KM/Archive/KaggleStuff/2017SBER/transformed_data/new/test_v1.csv')\n",
    "\n",
    "\n",
    "df_train = pd.merge(df_train, df_train_raw[['id', 'floor']], on='id')\n",
    "df_test = pd.merge(df_test, df_test_raw[['id', 'floor']], on='id')   \n",
    "\n",
    "df_train.to_csv('/Volumes/KM/Archive/KaggleStuff/2017SBER/transformed_data/new/train_v2.csv', index=False)\n",
    "df_test.to_csv('/Volumes/KM/Archive/KaggleStuff/2017SBER/transformed_data/new/test_v2.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('/Volumes/KM/Archive/KaggleStuff/2017SBER/transformed_data/new/train_v2.csv')\n",
    "df_test = pd.read_csv('/Volumes/KM/Archive/KaggleStuff/2017SBER/transformed_data/new/test_v2.csv')\n",
    "\n",
    "df_train = pd.merge(df_train, df_train_raw[['id', 'max_floor', 'num_room', 'kitch_sq']], on='id')\n",
    "df_test = pd.merge(df_test, df_test_raw[['id', 'max_floor', 'num_room', 'kitch_sq']], on='id')   \n",
    "\n",
    "df_train = df_train[~pd.isnull(df_train['floor'])]\n",
    "\n",
    "df_train['max_floor_lg1'] = df_train['max_floor']\n",
    "df_test['max_floor_lg1'] = df_test['max_floor']\n",
    "\n",
    "nan_ind = ~pd.isnull(df_train['max_floor_lg1'])\n",
    "\n",
    "lgreg_life_sq = LinearRegression(fit_intercept=True)\n",
    "\n",
    "tmp_feat_arr = np.zeros([df_train['full_sq'][nan_ind].values.shape[0], 3])\n",
    "tmp_feat_arr[:, 0] = df_train['price_eur'][nan_ind].values[:]**0.5\n",
    "tmp_feat_arr[:, 1] = df_train['floor'][nan_ind].values[:]**0.5\n",
    "tmp_feat_arr[:, 2] = df_train['sadovoe_km'][nan_ind].values[:]\n",
    "tmp_targ_arr = np.reshape(df_train['max_floor_lg1'][nan_ind].values,\n",
    "                          (df_train['max_floor_lg1'][nan_ind].values.shape[0], 1))\n",
    "lgreg_life_sq.fit(tmp_feat_arr, np.log(tmp_targ_arr + 1))\n",
    "\n",
    "tmp_feat_arr = np.zeros([df_train['full_sq'][~nan_ind].values.shape[0], 3])\n",
    "tmp_feat_arr[:, 0] = df_train['price_eur'][~nan_ind].values[:]**0.5\n",
    "tmp_feat_arr[:, 1] = df_train['floor'][~nan_ind].values[:]**0.5\n",
    "tmp_feat_arr[:, 2] = df_train['sadovoe_km'][~nan_ind].values[:]\n",
    "\n",
    "pred_lsq = lgreg_life_sq.predict(tmp_feat_arr)\n",
    "df_train.loc[~nan_ind, 'max_floor_lg1'] = np.exp(pred_lsq[:, 0]) - 1\n",
    "indarr = df_train['max_floor_lg1'] < df_train['floor']\n",
    "df_train.loc[indarr, 'max_floor_lg1'] = df_train.loc[indarr, 'floor']\n",
    "df_train.loc[df_train['max_floor_lg1'] < 1, 'max_floor_lg1'] = 1\n",
    "\n",
    "df_train['kitch_sq_lg1'] = df_train['kitch_sq']\n",
    "df_test['kitch_sq_lg1'] = df_test['kitch_sq']\n",
    "\n",
    "\n",
    "\n",
    "lgreg_life_sq = LinearRegression(fit_intercept=True)\n",
    "\n",
    "tmp_feat_arr = np.zeros([df_train['full_sq'][nan_ind].values.shape[0], 7])\n",
    "tmp_feat_arr[:, 0] = df_train['life_sq_lg1'][nan_ind].values[:]\n",
    "tmp_feat_arr[:, 1] = df_train['full_sq'][nan_ind].values[:]\n",
    "tmp_feat_arr[:, 2] = df_train['life_sq_lg1'][nan_ind].values[:]\n",
    "tmp_feat_arr[:, 3] = df_train['life_sq_lg1'][nan_ind].values[:] / (df_train['full_sq'][nan_ind].values[:] + 1)\n",
    "tmp_feat_arr[:, 4] = df_train['ekder_all'][nan_ind].values[:]\n",
    "tmp_feat_arr[:, 5] = df_train['office_raion'][nan_ind].values[:]\n",
    "tmp_feat_arr[:, 6] = df_train['sport_count_3000'][nan_ind].values[:]\n",
    "tmp_targ_arr = np.reshape(df_train['kitch_sq_lg1'][nan_ind].values,\n",
    "                          (df_train['kitch_sq_lg1'][nan_ind].values.shape[0], 1))\n",
    "lgreg_life_sq.fit(tmp_feat_arr, np.log(tmp_targ_arr + 1))\n",
    "\n",
    "tmp_feat_arr = np.zeros([df_train['full_sq'][~nan_ind].values.shape[0], 7])\n",
    "tmp_feat_arr[:, 0] = df_train['life_sq_lg1'][~nan_ind].values[:]\n",
    "tmp_feat_arr[:, 1] = df_train['full_sq'][~nan_ind].values[:]\n",
    "tmp_feat_arr[:, 2] = df_train['life_sq_lg1'][~nan_ind].values[:]\n",
    "tmp_feat_arr[:, 3] = df_train['life_sq_lg1'][~nan_ind].values[:] / (df_train['full_sq'][~nan_ind].values[:] + 1)\n",
    "tmp_feat_arr[:, 4] = df_train['ekder_all'][~nan_ind].values[:]\n",
    "tmp_feat_arr[:, 5] = df_train['office_raion'][~nan_ind].values[:]\n",
    "tmp_feat_arr[:, 6] = df_train['sport_count_3000'][~nan_ind].values[:]\n",
    "\n",
    "pred_lsq = lgreg_life_sq.predict(tmp_feat_arr)\n",
    "df_train.loc[~nan_ind, 'kitch_sq_lg1'] = np.exp(pred_lsq[:, 0]) - 1\n",
    "df_train.loc[df_train['kitch_sq_lg1'] < 0, 'kitch_sq_lg1'] = 0\n",
    "\n",
    "df_train.to_csv('/Volumes/KM/Archive/KaggleStuff/2017SBER/transformed_data/new/train_v3.csv', index=False)\n",
    "df_test.to_csv('/Volumes/KM/Archive/KaggleStuff/2017SBER/transformed_data/new/test_v3.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/georgeoblapenko/anaconda/lib/python3.5/site-packages/ipykernel/__main__.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/Users/georgeoblapenko/anaconda/lib/python3.5/site-packages/ipykernel/__main__.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/Users/georgeoblapenko/anaconda/lib/python3.5/site-packages/sklearn/preprocessing/label.py:112: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/georgeoblapenko/anaconda/lib/python3.5/site-packages/sklearn/preprocessing/label.py:147: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "df_train = pd.read_csv('/Volumes/KM/Archive/KaggleStuff/2017SBER/transformed_data/new/train_v3.csv')\n",
    "df_test = pd.read_csv('/Volumes/KM/Archive/KaggleStuff/2017SBER/transformed_data/new/test_v3.csv')\n",
    "\n",
    "df_train = pd.merge(df_train, df_train_raw[['id', 'state', 'material', 'build_year']], on='id')\n",
    "df_test = pd.merge(df_test, df_test_raw[['id', 'state', 'material', 'build_year']], on='id')   \n",
    "\n",
    "df_train['build_year'][pd.isnull(df_train['build_year'])] = 0\n",
    "df_test['build_year'][pd.isnull(df_test['build_year'])] = 0\n",
    "# pd.isnull(df_train['build_year']).sum()\n",
    "\n",
    "df_train['unbuilt'] = (df_train['build_year'] > df_train['sale_year']).apply(lambda x: 1 if x else 0)\n",
    "df_test['unbuilt'] = (df_test['build_year'] > df_test['sale_year']).apply(lambda x: 1 if x else 0)\n",
    "\n",
    "df_train = df_train[df_train['state'] != 33.0] \n",
    "\n",
    "nan_ind = ~pd.isnull(df_train['state'])\n",
    "clf = XGBClassifier(max_depth=4, n_estimators=500, learning_rate=0.05)\n",
    "\n",
    "\n",
    "\n",
    "df_train['state_xg1'] = df_train['state']\n",
    "df_test['state_xg1'] = df_test['state']\n",
    "\n",
    "\n",
    "\n",
    "nan_ind = ~pd.isnull(df_train['state_xg1'])\n",
    "\n",
    "lgreg_life_sq = LinearRegression(fit_intercept=True)\n",
    "\n",
    "tmp_feat_arr = np.zeros([df_train['state_xg1'][nan_ind].values.shape[0], 4])\n",
    "tmp_feat_arr[:, 0] = df_train['ekder_all'][nan_ind].values[:]\n",
    "tmp_feat_arr[:, 1] = df_train['thermal_power_plant_km'][nan_ind].values[:]\n",
    "tmp_feat_arr[:, 2] = df_train['full_sq'][nan_ind].values[:]\n",
    "tmp_feat_arr[:, 3] = df_train['kitch_sq_lg1'][nan_ind].values[:]\n",
    "tmp_targ_arr = np.reshape(df_train['state_xg1'][nan_ind].values,\n",
    "                          (df_train['state_xg1'][nan_ind].values.shape[0], 1))\n",
    "clf.fit(tmp_feat_arr, tmp_targ_arr)\n",
    "tmp_feat_arr = np.zeros([df_train['state_xg1'][~nan_ind].values.shape[0], 4])\n",
    "tmp_feat_arr[:, 0] = df_train['ekder_all'][~nan_ind].values[:]\n",
    "tmp_feat_arr[:, 1] = df_train['thermal_power_plant_km'][~nan_ind].values[:]\n",
    "tmp_feat_arr[:, 2] = df_train['full_sq'][~nan_ind].values[:]\n",
    "tmp_feat_arr[:, 3] = df_train['kitch_sq_lg1'][~nan_ind].values[:]\n",
    "\n",
    "pred = clf.predict(tmp_feat_arr)\n",
    "df_train.loc[~nan_ind, 'state_xg1'] = pred\n",
    "# df_train.loc[df_train['life_sq_lg1'] < 0, 'life_sq_lg1'] = 0\n",
    "\n",
    "nan_ind = ~pd.isnull(df_test['state_xg1'])\n",
    "tmp_feat_arr = np.zeros([df_test['state_xg1'][~nan_ind].values.shape[0], 4])\n",
    "tmp_feat_arr[:, 0] = df_test['ekder_all'][~nan_ind].values[:]\n",
    "tmp_feat_arr[:, 1] = df_test['thermal_power_plant_km'][~nan_ind].values[:]\n",
    "tmp_feat_arr[:, 2] = df_test['full_sq'][~nan_ind].values[:]\n",
    "tmp_feat_arr[:, 3] = df_test['kitch_sq_lg1'][~nan_ind].values[:]\n",
    "\n",
    "pred = clf.predict(tmp_feat_arr)\n",
    "df_test.loc[~nan_ind, 'state_xg1'] = pred\n",
    "\n",
    "state_enc = OneHotEncoder(sparse=False)\n",
    "state_onehot = state_enc.fit_transform(df_train['state_xg1'].values.reshape(df_train['state_xg1'].values.shape[0], 1))\n",
    "\n",
    "for ii in range(state_onehot.shape[1]):\n",
    "    df_train['state_'+str(ii)] = state_onehot[:, ii]\n",
    "    \n",
    "state_onehot = state_enc.transform(df_test['state_xg1'].values.reshape(df_test['state_xg1'].values.shape[0], 1))\n",
    "\n",
    "for ii in range(state_onehot.shape[1]):\n",
    "    df_test['state_'+str(ii)] = state_onehot[:, ii]\n",
    "    \n",
    "df_train.to_csv('/Volumes/KM/Archive/KaggleStuff/2017SBER/transformed_data/new/train_v4.csv', index=False)\n",
    "df_test.to_csv('/Volumes/KM/Archive/KaggleStuff/2017SBER/transformed_data/new/test_v4.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are some cells which were used to test various missing data reconstruction approaches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/georgeoblapenko/anaconda/lib/python3.5/site-packages/sklearn/preprocessing/label.py:112: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/georgeoblapenko/anaconda/lib/python3.5/site-packages/sklearn/preprocessing/label.py:147: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/georgeoblapenko/anaconda/lib/python3.5/site-packages/sklearn/preprocessing/label.py:112: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/georgeoblapenko/anaconda/lib/python3.5/site-packages/sklearn/preprocessing/label.py:147: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/georgeoblapenko/anaconda/lib/python3.5/site-packages/sklearn/preprocessing/label.py:112: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/georgeoblapenko/anaconda/lib/python3.5/site-packages/sklearn/preprocessing/label.py:147: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[['max_floor_lg1', 0.75740804191186351]]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = df_train[~pd.isnull(df_train['state'])]\n",
    "\n",
    "nan_ind = ~pd.isnull(df_train['state'])\n",
    "\n",
    "# lgreg_life_sq = LinearRegression(fit_intercept=True)\n",
    "# lgreg_life_sq = XGBRegressor(max_depth=2, learning_rate=0.01, n_estimators=600, subsample=1., gamma=0.0)\n",
    "clf = XGBClassifier(max_depth=4, n_estimators=500, learning_rate=0.05)\n",
    "# clf = GradientBoostingClassifier()\n",
    "\n",
    "last_ind = 4\n",
    "\n",
    "tmp_feat_arr = np.zeros([df_train['ekder_all'][nan_ind].values.shape[0], last_ind])\n",
    "\n",
    "\n",
    "tmp_feat_arr[:, 0] = df_train['ekder_all'][nan_ind].values[:]\n",
    "tmp_feat_arr[:, 1] = df_train['thermal_power_plant_km'][nan_ind].values[:]\n",
    "tmp_feat_arr[:, 2] = df_train['full_sq'][nan_ind].values[:]\n",
    "tmp_feat_arr[:, 3] = df_train['kitch_sq_lg1'][nan_ind].values[:]\n",
    "output_res = []\n",
    "\n",
    "# cols_list = []\n",
    "# for col in cols_list:\n",
    "#     tmp_feat_arr[:, last_ind - 1] = df_train[col][nan_ind].values[:]**1.\n",
    "\n",
    "tmp_targ_arr = np.reshape(df_train['state'][nan_ind].values,\n",
    "                          (df_train['state'][nan_ind].values.shape[0], 1))\n",
    "n_splits = 3\n",
    "kf = StratifiedKFold(n_splits=n_splits)\n",
    "res = []\n",
    "for train_index, test_index in kf.split(tmp_feat_arr, tmp_targ_arr.ravel()):\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(tmp_feat_arr[train_index])\n",
    "\n",
    "    train_features = scaler.transform(tmp_feat_arr[train_index])\n",
    "    nel = tmp_targ_arr[train_index].shape[0]\n",
    "    clf.fit(train_features, tmp_targ_arr[train_index])\n",
    "    pred_pr = clf.predict_proba(scaler.transform(tmp_feat_arr[test_index]))\n",
    "    nel = tmp_targ_arr[test_index].shape[0]\n",
    "#         print((tmp_targ_arr[test_index] == 0).sum(), (tmp_targ_arr[test_index] == 1).sum())\n",
    "    res.append(log_loss(tmp_targ_arr[test_index], pred_pr))\n",
    "\n",
    "output_res.append([col, np.sum(res) / n_splits])\n",
    "\n",
    "sorted(output_res, key=lambda x: x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['sadovoe_km', 0.68258484628061311],\n",
       " ['bulvar_ring_km', 0.68267852629789894],\n",
       " ['full_sq', 0.68405072657366195]]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = df_train[~pd.isnull(df_train['floor'])]\n",
    "\n",
    "nan_ind = ~pd.isnull(df_train['max_floor'])\n",
    "\n",
    "lgreg_life_sq = LinearRegression(fit_intercept=True)\n",
    "# lgreg_life_sq = XGBRegressor(max_depth=2, learning_rate=0.01, n_estimators=600, subsample=1., gamma=0.0)\n",
    "\n",
    "last_ind = 4\n",
    "\n",
    "tmp_feat_arr = np.zeros([df_train['full_sq'][nan_ind].values.shape[0], last_ind])\n",
    "tmp_feat_arr[:, 0] = df_train['price_eur'][nan_ind].values[:]**0.5\n",
    "tmp_feat_arr[:, 1] = df_train['floor'][nan_ind].values[:]**0.5\n",
    "# tmp_feat_arr[:, 2] = df_train['full_sq'][nan_ind].values[:]**0.15\n",
    "# tmp_feat_arr[:, 1] = df_train['kremlin_km'][nan_ind].values[:] ** 0.8\n",
    "# tmp_feat_arr[:, 2] = df_train['workplaces_km'][nan_ind].values[:]\n",
    "# tmp_feat_arr[:, 3] = df_train['church_synagogue_km'][nan_ind].values[:]\n",
    "# # tmp_feat_arr[:, 4] = df_train['trc_sqm_5000'][nan_ind].values[:]**2.5\n",
    "# tmp_feat_arr[:, 5] = df_train['hospice_morgue_km'][nan_ind].values[:]**1.5\n",
    "\n",
    "output_res = []\n",
    "\n",
    "cols_list = ['full_sq', 'sadovoe_km', 'bulvar_ring_km', ]\n",
    "\n",
    "for col in cols_list:\n",
    "    tmp_feat_arr[:, last_ind - 1] = df_train[col][nan_ind].values[:]**1.\n",
    "\n",
    "    tmp_targ_arr = np.reshape(df_train['max_floor'][nan_ind].values,\n",
    "                              (df_train['max_floor'][nan_ind].values.shape[0], 1))\n",
    "    n_splits = 4\n",
    "    kf = KFold(n_splits=n_splits)\n",
    "    res = []\n",
    "    for train_index, test_index in kf.split(tmp_feat_arr):\n",
    "        scaler = StandardScaler()\n",
    "        scaler.fit(tmp_feat_arr[train_index])\n",
    "\n",
    "        train_features = scaler.transform(tmp_feat_arr[train_index])\n",
    "\n",
    "        lgreg_life_sq.fit(train_features, np.log(tmp_targ_arr[train_index] + 1))\n",
    "        pred = lgreg_life_sq.predict(scaler.transform(tmp_feat_arr[test_index]))\n",
    "\n",
    "        pred = np.exp(pred) - 1\n",
    "#         res.append(np.sqrt(np.mean(np.square(np.log(pred + 1) - np.log(tmp_targ_arr[test_index] + 1)))))\n",
    "        res.append(np.sqrt(np.mean(np.square(np.log(pred + 1) - np.log(tmp_targ_arr[test_index] + 1)))))\n",
    "#     print(res, np.sum(res) / n_splits)\n",
    "    output_res.append([col, np.sum(res) / n_splits])\n",
    "#     print(col, np.sum(res) / n_splits)\n",
    "sorted(output_res, key=lambda x: x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['sport_count_3000', 0.6952740393938861],\n",
       " ['sport_count_5000', 0.69554000134020255],\n",
       " ['trc_count_5000', 0.6958130703714942],\n",
       " ['sport_count_2000', 0.69801556209740245],\n",
       " ['sport_count_1500', 0.69965608335033691],\n",
       " ['trc_sqm_5000', 0.69969018833236019],\n",
       " ['market_count_5000', 0.70163349535839725],\n",
       " ['cafe_count_5000_price_1000', 0.70245196477818594],\n",
       " ['cafe_count_5000_price_500', 0.70247545372726472],\n",
       " ['trc_count_3000', 0.70250754837103357],\n",
       " ['cafe_count_5000_na_price', 0.70283095461487921],\n",
       " ['office_count_5000', 0.70317755370223334],\n",
       " ['cafe_count_5000', 0.7035759005064709],\n",
       " ['cafe_count_5000_price_1500', 0.70383113113769724],\n",
       " ['office_sqm_5000', 0.70400490196135623],\n",
       " ['big_church_count_5000', 0.70433399431501564],\n",
       " ['sport_count_1000', 0.70442632532885496],\n",
       " ['office_sqm_3000', 0.70448668996528907],\n",
       " ['church_count_5000', 0.70457526639150947],\n",
       " ['zd_vokzaly_avto_km', 0.70467158022617415],\n",
       " ['workplaces_km', 0.7047980026950863],\n",
       " ['trc_sqm_3000', 0.70511158153291209],\n",
       " ['fitness_km', 0.70511974302910474],\n",
       " ['university_km', 0.70604814589817932],\n",
       " ['detention_facility_km', 0.70632695026839265],\n",
       " ['cafe_count_5000_price_2500', 0.70632789246252581],\n",
       " ['catering_km', 0.70658235724843277],\n",
       " ['theater_km', 0.70675390546527928],\n",
       " ['leisure_count_5000', 0.7068774914399899],\n",
       " ['sport_objects_raion', 0.70714025432169403],\n",
       " ['office_sqm_2000', 0.70737501038705486],\n",
       " ['nuclear_reactor_km', 0.70737538260928745],\n",
       " ['sadovoe_km', 0.70764759881573869],\n",
       " ['bulvar_ring_km', 0.70768365895484819],\n",
       " ['kremlin_km', 0.70808292989050736],\n",
       " ['ttk_km', 0.70813830023907842],\n",
       " ['cafe_count_5000_price_4000', 0.70828583023165559],\n",
       " ['office_km', 0.70834834329536478],\n",
       " ['swim_pool_km', 0.70853643293917168],\n",
       " ['office_sqm_1500', 0.70869764951347769],\n",
       " ['public_healthcare_km', 0.70895102707029367],\n",
       " ['basketball_km', 0.70905725804053943],\n",
       " ['office_count_1500', 0.70933911091117863],\n",
       " ['cafe_count_5000_price_high', 0.70937775659869229],\n",
       " ['big_church_km', 0.70944789832537947],\n",
       " ['trc_count_2000', 0.70962881555667723],\n",
       " ['thermal_power_plant_km', 0.70987800969167503],\n",
       " ['cafe_count_3000_price_1000', 0.710067975568761],\n",
       " ['indust_part', 0.71026712815015314],\n",
       " ['school_education_centers_raion', 0.71027606138467148],\n",
       " ['shopping_centers_km', 0.71034821330184617],\n",
       " ['stadium_km', 0.71036161393639652],\n",
       " ['raion_popul', 0.7105168338038037],\n",
       " ['ekder_male', 0.71055511373644786],\n",
       " ['ekder_female', 0.71055622133610519],\n",
       " ['park_km', 0.71072822255237333],\n",
       " ['cafe_count_2000', 0.71083740579690957],\n",
       " ['metro_min_avto', 0.71089228771851132],\n",
       " ['full_sq', 0.71092867606214138],\n",
       " ['metro_km_avto', 0.71093536433297277],\n",
       " ['exhibition_km', 0.71093750777585774],\n",
       " ['office_raion', 0.71096580156368405],\n",
       " ['eurrub', 0.71254885035195747],\n",
       " ['usdrub', 0.71306266632773152]]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targ = 'kitch_sq'\n",
    "\n",
    "\n",
    "df_train = df_train[~pd.isnull(df_train['floor'])]\n",
    "\n",
    "nan_ind = ~pd.isnull(df_train[targ])\n",
    "\n",
    "lgreg_life_sq = LinearRegression(fit_intercept=True)\n",
    "# lgreg_life_sq = XGBRegressor(max_depth=2, learning_rate=0.01, n_estimators=600, subsample=1., gamma=0.0)\n",
    "\n",
    "last_ind = 7\n",
    "\n",
    "tmp_feat_arr = np.zeros([df_train['full_sq'][nan_ind].values.shape[0], last_ind])\n",
    "tmp_feat_arr[:, 0] = df_train['price_eur'][nan_ind].values[:]\n",
    "tmp_feat_arr[:, 1] = df_train['life_sq_lg1'][nan_ind].values[:]\n",
    "tmp_feat_arr[:, 2] = df_train['full_sq'][nan_ind].values[:]\n",
    "tmp_feat_arr[:, 3] = df_train['life_sq_lg1'][nan_ind].values[:] / (df_train['full_sq'][nan_ind].values[:] + 1)\n",
    "tmp_feat_arr[:, 4] = df_train['ekder_all'][nan_ind].values[:]\n",
    "tmp_feat_arr[:, 5] = df_train['office_raion'][nan_ind].values[:]\n",
    "tmp_feat_arr[:, 6] = df_train['sport_count_3000'][nan_ind].values[:]\n",
    "# tmp_feat_arr[:, 1] = df_train['kremlin_km'][nan_ind].values[:] ** 0.8\n",
    "# tmp_feat_arr[:, 2] = df_train['workplaces_km'][nan_ind].values[:]\n",
    "# tmp_feat_arr[:, 3] = df_train['church_synagogue_km'][nan_ind].values[:]\n",
    "# # tmp_feat_arr[:, 4] = df_train['trc_sqm_5000'][nan_ind].values[:]**2.5\n",
    "# tmp_feat_arr[:, 5] = df_train['hospice_morgue_km'][nan_ind].values[:]**1.5\n",
    "\n",
    "output_res = []\n",
    "\n",
    "cols_list = ['full_sq', 'sport_count_5000', 'sport_count_3000', 'trc_count_5000', 'zd_vokzaly_avto_km',\n",
    "                'sadovoe_km', 'sport_count_2000', 'bulvar_ring_km', 'kremlin_km', 'ttk_km', 'trc_sqm_5000',\n",
    "                'nuclear_reactor_km', 'sport_count_1500', 'office_sqm_5000', 'sport_objects_raion', 'trc_count_3000',\n",
    "                'stadium_km', 'cafe_count_5000_price_1000', 'detention_facility_km', 'basketball_km',\n",
    "                'cafe_count_5000_price_1500', 'office_km', 'cafe_count_5000', 'cafe_count_5000_na_price',\n",
    "                'university_km', 'trc_sqm_3000', 'cafe_count_5000_price_500', 'workplaces_km',\n",
    "                'cafe_count_5000_price_2500', 'office_sqm_3000', 'swim_pool_km', 'thermal_power_plant_km',\n",
    "                'office_count_5000', 'catering_km', 'exhibition_km', 'church_count_5000', 'office_sqm_2000',\n",
    "                'cafe_count_5000_price_high', 'cafe_count_5000_price_4000', 'big_church_km',\n",
    "                'school_education_centers_raion', 'sport_count_1000', 'fitness_km', 'metro_min_avto',\n",
    "                'market_count_5000', 'park_km', 'big_church_count_5000', 'leisure_count_5000',\n",
    "                'office_sqm_1500', 'ekder_male', 'metro_km_avto', 'trc_count_2000', 'shopping_centers_km',\n",
    "                'public_healthcare_km', 'ekder_female', 'cafe_count_3000_price_1000',\n",
    "                'office_count_1500', 'raion_popul', 'usdrub', 'eurrub', 'cafe_count_2000', 'theater_km',\n",
    "                'office_raion', 'indust_part']\n",
    "\n",
    "for col in cols_list:\n",
    "    tmp_feat_arr[:, last_ind - 1] = df_train[col][nan_ind].values[:]**1.\n",
    "\n",
    "    tmp_targ_arr = np.reshape(df_train[targ][nan_ind].values,\n",
    "                              (df_train[targ][nan_ind].values.shape[0], 1))\n",
    "    n_splits = 4\n",
    "    kf = KFold(n_splits=n_splits)\n",
    "    res = []\n",
    "    for train_index, test_index in kf.split(tmp_feat_arr):\n",
    "        scaler = StandardScaler()\n",
    "        scaler.fit(tmp_feat_arr[train_index])\n",
    "\n",
    "        train_features = scaler.transform(tmp_feat_arr[train_index])\n",
    "\n",
    "        lgreg_life_sq.fit(train_features, np.log(tmp_targ_arr[train_index] + 1))\n",
    "        pred = lgreg_life_sq.predict(scaler.transform(tmp_feat_arr[test_index]))\n",
    "\n",
    "        pred = np.exp(pred) - 1\n",
    "#         res.append(np.sqrt(np.mean(np.square(np.log(pred + 1) - np.log(tmp_targ_arr[test_index] + 1)))))\n",
    "        res.append(np.sqrt(np.mean(np.square(np.log(pred + 1) - np.log(tmp_targ_arr[test_index] + 1)))))\n",
    "#     print(res, np.sum(res) / n_splits)\n",
    "    output_res.append([col, np.sum(res) / n_splits])\n",
    "#     print(col, np.sum(res) / n_splits)\n",
    "sorted(output_res, key=lambda x: x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['full_sq', 0.20291984253450826],\n",
       " ['sport_count_5000', 0.20291984253450826],\n",
       " ['sport_count_3000', 0.20291984253450826],\n",
       " ['trc_count_5000', 0.20291984253450826],\n",
       " ['zd_vokzaly_avto_km', 0.20291984253450826],\n",
       " ['sadovoe_km', 0.20291984253450826],\n",
       " ['sport_count_2000', 0.20291984253450826],\n",
       " ['bulvar_ring_km', 0.20291984253450826],\n",
       " ['kremlin_km', 0.20291984253450826],\n",
       " ['ttk_km', 0.20291984253450826],\n",
       " ['trc_sqm_5000', 0.20291984253450826],\n",
       " ['nuclear_reactor_km', 0.20291984253450826],\n",
       " ['sport_count_1500', 0.20291984253450826],\n",
       " ['office_sqm_5000', 0.20291984253450826],\n",
       " ['sport_objects_raion', 0.20291984253450826],\n",
       " ['trc_count_3000', 0.20291984253450826],\n",
       " ['stadium_km', 0.20291984253450826],\n",
       " ['cafe_count_5000_price_1000', 0.20291984253450826],\n",
       " ['detention_facility_km', 0.20291984253450826],\n",
       " ['basketball_km', 0.20291984253450826],\n",
       " ['cafe_count_5000_price_1500', 0.20291984253450826],\n",
       " ['office_km', 0.20291984253450826],\n",
       " ['cafe_count_5000', 0.20291984253450826],\n",
       " ['cafe_count_5000_na_price', 0.20291984253450826],\n",
       " ['university_km', 0.20291984253450826],\n",
       " ['trc_sqm_3000', 0.20291984253450826],\n",
       " ['cafe_count_5000_price_500', 0.20291984253450826],\n",
       " ['workplaces_km', 0.20291984253450826],\n",
       " ['cafe_count_5000_price_2500', 0.20291984253450826],\n",
       " ['office_sqm_3000', 0.20291984253450826],\n",
       " ['swim_pool_km', 0.20291984253450826],\n",
       " ['thermal_power_plant_km', 0.20291984253450826],\n",
       " ['office_count_5000', 0.20291984253450826],\n",
       " ['catering_km', 0.20291984253450826],\n",
       " ['exhibition_km', 0.20291984253450826],\n",
       " ['church_count_5000', 0.20291984253450826],\n",
       " ['office_sqm_2000', 0.20291984253450826],\n",
       " ['cafe_count_5000_price_high', 0.20291984253450826],\n",
       " ['cafe_count_5000_price_4000', 0.20291984253450826],\n",
       " ['big_church_km', 0.20291984253450826],\n",
       " ['school_education_centers_raion', 0.20291984253450826],\n",
       " ['sport_count_1000', 0.20291984253450826],\n",
       " ['fitness_km', 0.20291984253450826],\n",
       " ['metro_min_avto', 0.20291984253450826],\n",
       " ['market_count_5000', 0.20291984253450826],\n",
       " ['park_km', 0.20291984253450826],\n",
       " ['big_church_count_5000', 0.20291984253450826],\n",
       " ['leisure_count_5000', 0.20291984253450826],\n",
       " ['office_sqm_1500', 0.20291984253450826],\n",
       " ['ekder_male', 0.20291984253450826],\n",
       " ['metro_km_avto', 0.20291984253450826],\n",
       " ['trc_count_2000', 0.20291984253450826],\n",
       " ['shopping_centers_km', 0.20291984253450826],\n",
       " ['public_healthcare_km', 0.20291984253450826],\n",
       " ['ekder_female', 0.20291984253450826],\n",
       " ['cafe_count_3000_price_1000', 0.20291984253450826],\n",
       " ['office_count_1500', 0.20291984253450826],\n",
       " ['raion_popul', 0.20291984253450826],\n",
       " ['usdrub', 0.20291984253450826],\n",
       " ['eurrub', 0.20291984253450826],\n",
       " ['cafe_count_2000', 0.20291984253450826],\n",
       " ['theater_km', 0.20291984253450826],\n",
       " ['office_raion', 0.20291984253450826],\n",
       " ['indust_part', 0.20291984253450826]]"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targ = 'num_room'\n",
    "\n",
    "\n",
    "df_train = df_train[~pd.isnull(df_train['floor'])]\n",
    "\n",
    "nan_ind = ~pd.isnull(df_train[targ])\n",
    "\n",
    "lgreg_life_sq = LinearRegression(fit_intercept=True)\n",
    "# lgreg_life_sq = XGBRegressor(max_depth=2, learning_rate=0.01, n_estimators=600, subsample=1., gamma=0.0)\n",
    "\n",
    "last_ind = 7\n",
    "\n",
    "tmp_feat_arr = np.zeros([df_train['full_sq'][nan_ind].values.shape[0], last_ind])\n",
    "tmp_feat_arr[:, 0] = df_train['price_eur'][nan_ind].values[:]\n",
    "tmp_feat_arr[:, 1] = df_train['life_sq_lg1'][nan_ind].values[:]\n",
    "tmp_feat_arr[:, 2] = df_train['full_sq'][nan_ind].values[:]\n",
    "# tmp_feat_arr[:, 3] = df_train['kitch_sq_lg1'][nan_ind].values[:]\n",
    "# tmp_feat_arr[:, 4] = df_train['life_sq_lg1'][nan_ind].values[:] / (df_train['full_sq'][nan_ind].values[:] + 1)\n",
    "# tmp_feat_arr[:, 1] = df_train['kremlin_km'][nan_ind].values[:] ** 0.8\n",
    "# tmp_feat_arr[:, 2] = df_train['workplaces_km'][nan_ind].values[:]\n",
    "# tmp_feat_arr[:, 3] = df_train['church_synagogue_km'][nan_ind].values[:]\n",
    "# # tmp_feat_arr[:, 4] = df_train['trc_sqm_5000'][nan_ind].values[:]**2.5\n",
    "# tmp_feat_arr[:, 5] = df_train['hospice_morgue_km'][nan_ind].values[:]**1.5\n",
    "\n",
    "output_res = []\n",
    "\n",
    "cols_list = ['full_sq', 'sport_count_5000', 'sport_count_3000', 'trc_count_5000', 'zd_vokzaly_avto_km',\n",
    "                'sadovoe_km', 'sport_count_2000', 'bulvar_ring_km', 'kremlin_km', 'ttk_km', 'trc_sqm_5000',\n",
    "                'nuclear_reactor_km', 'sport_count_1500', 'office_sqm_5000', 'sport_objects_raion', 'trc_count_3000',\n",
    "                'stadium_km', 'cafe_count_5000_price_1000', 'detention_facility_km', 'basketball_km',\n",
    "                'cafe_count_5000_price_1500', 'office_km', 'cafe_count_5000', 'cafe_count_5000_na_price',\n",
    "                'university_km', 'trc_sqm_3000', 'cafe_count_5000_price_500', 'workplaces_km',\n",
    "                'cafe_count_5000_price_2500', 'office_sqm_3000', 'swim_pool_km', 'thermal_power_plant_km',\n",
    "                'office_count_5000', 'catering_km', 'exhibition_km', 'church_count_5000', 'office_sqm_2000',\n",
    "                'cafe_count_5000_price_high', 'cafe_count_5000_price_4000', 'big_church_km',\n",
    "                'school_education_centers_raion', 'sport_count_1000', 'fitness_km', 'metro_min_avto',\n",
    "                'market_count_5000', 'park_km', 'big_church_count_5000', 'leisure_count_5000',\n",
    "                'office_sqm_1500', 'ekder_male', 'metro_km_avto', 'trc_count_2000', 'shopping_centers_km',\n",
    "                'public_healthcare_km', 'ekder_female', 'cafe_count_3000_price_1000',\n",
    "                'office_count_1500', 'raion_popul', 'usdrub', 'eurrub', 'cafe_count_2000', 'theater_km',\n",
    "                'office_raion', 'indust_part']\n",
    "\n",
    "for col in cols_list:\n",
    "#     tmp_feat_arr[:, last_ind - 1] = df_train[col][nan_ind].values[:]**1.\n",
    "\n",
    "    tmp_targ_arr = np.reshape(df_train[targ][nan_ind].values,\n",
    "                              (df_train[targ][nan_ind].values.shape[0], 1))\n",
    "    n_splits = 4\n",
    "    kf = KFold(n_splits=n_splits)\n",
    "    res = []\n",
    "    for train_index, test_index in kf.split(tmp_feat_arr):\n",
    "        scaler = StandardScaler()\n",
    "        scaler.fit(tmp_feat_arr[train_index])\n",
    "\n",
    "        train_features = scaler.transform(tmp_feat_arr[train_index])\n",
    "\n",
    "        lgreg_life_sq.fit(train_features, np.log(tmp_targ_arr[train_index] + 1))\n",
    "        pred = lgreg_life_sq.predict(scaler.transform(tmp_feat_arr[test_index]))\n",
    "\n",
    "        pred = np.exp(pred) - 1\n",
    "#         res.append(np.sqrt(np.mean(np.square(np.log(pred + 1) - np.log(tmp_targ_arr[test_index] + 1)))))\n",
    "        res.append(np.sqrt(np.mean(np.square(np.log(pred + 1) - np.log(tmp_targ_arr[test_index] + 1)))))\n",
    "#     print(res, np.sum(res) / n_splits)\n",
    "    output_res.append([col, np.sum(res) / n_splits])\n",
    "#     print(col, np.sum(res) / n_splits)\n",
    "sorted(output_res, key=lambda x: x[1])"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
